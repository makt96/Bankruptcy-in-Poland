{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f72319d-53df-406a-938f-44ea18e426d0",
   "metadata": {},
   "source": [
    "<font size=\"+3\"><strong>Imbalanced Data</strong></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af850f1d-c25f-4ad2-bad2-998e3857b111",
   "metadata": {},
   "source": [
    "In the last lesson, we prepared the data. \n",
    "\n",
    "In this lesson, we're going to explore some of the features of the dataset, use visualizations to help us understand those features, and develop a model that solves the problem of imbalanced data by under- and over-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d22b499-7de4-4055-8613-fc19f078d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7dcfe-a2f9-4ce1-bacc-3ba7afacece2",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcca9662-9a40-4fcc-8cde-af9555c48f18",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aaa7b7-9e6c-4986-8f8b-332e8ba0e358",
   "metadata": {},
   "source": [
    "As always, we need to begin by bringing our data into the project, and the function we developed in the previous module is exactly what we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2977e0a6-eccf-4bdf-b184-eb70b0d841c8",
   "metadata": {},
   "source": [
    " Complete the `wrangle` function below using the code you developed in the last lesson. Then use it to import `poland-bankruptcy-data-2009.json.gz` into the DataFrame `df`.\n",
    "\n",
    "- [<span id='technique'>Write a function in <span id='tool'>Python</span></span>.](../%40textbook/02-python-advanced.ipynb#Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb35777-50c4-43b1-829a-fda6f4de773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(filename):\n",
    "    \n",
    "    # Open compressed file, load into dictionary\n",
    "    with gzip.open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    # Load dictionary into DataFrame, set index\n",
    "    df = pd.DataFrame().from_dict(data[\"data\"]).set_index(\"company_id\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5eeba5-dad9-4e50-a8af-0cbc5c4f55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle(\"data/poland-bankruptcy-data-2009.json.gz\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9576d4f-b03d-4364-80ef-6108028d7fb7",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe95957e-fb6a-4aa9-ae09-e21e4d20b426",
   "metadata": {},
   "source": [
    "Let's take a moment to refresh our memory on what's in this dataset. In the last lesson, we noticed that the data was stored in a JSON file (similar to a Python dictionary), and we explored the key-value pairs. This time, we're going to look at what the values in those pairs actually are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476786a6-6f5b-4619-bfc3-654caa4cdf6c",
   "metadata": {},
   "source": [
    " Use the `info` method to explore `df`. What type of features does this dataset have? Which column is the target? Are there columns will missing values that we'll need to address?\n",
    "\n",
    "- [Inspect a DataFrame using the `shape`, `info`, and `head` in pandas.](../%40textbook/03-pandas-getting-started.ipynb#Inspecting-DataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2c636-fab4-4b04-8479-44e13e996608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect DataFrame\n",
    "#df.info()\n",
    "df.isnull().sum() / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e1dfc-24e6-444b-b18e-4a9035a28bde",
   "metadata": {},
   "source": [
    "That's solid information. We know all our features are numerical and that we have missing data. But, as always, it's a good idea to do some visualizations to see if there are any interesting trends or ideas we should keep in mind while we work. First, let's take a look at how many firms are bankrupt, and how many are not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80107e-5b15-40b5-ada6-32720a9ce274",
   "metadata": {},
   "source": [
    "Create a bar chart of the value counts for the `\"bankrupt\"` column. You want to calculate the relative frequencies of the classes, not the raw count, so be sure to set the `normalize` argument to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db28e0-e9fb-46e0-b1af-68e262fb391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class balance\n",
    "df[\"bankrupt\"].value_counts(normalize=True).plot(\n",
    "    kind=\"bar\",\n",
    "    xlabel=\"Bankrupt\",\n",
    "    ylabel=\"Frequency\",\n",
    "    title=\"Class Balance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e8b84f-d991-47f4-ae1d-0e74dfd56a16",
   "metadata": {},
   "source": [
    "That's good news for Poland's economy! Since it looks like most of the companies in our dataset are doing all right for themselves, let's drill down a little farther. However, it also shows us that we have an imbalanced dataset, where our majority class is far bigger than our minority class.\n",
    "\n",
    "In the last lesson, we saw that there were 64 features of each company, each of which had some kind of numerical value. It might be useful to understand where the values for one of these features cluster, so let's make a boxplot to see how the values in `\"feat_27\"` are distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ecf47-6cec-49b4-a559-3afd393fff40",
   "metadata": {},
   "source": [
    "Use seaborn to create a boxplot that shows the distributions of the `\"feat_27\"` column for both groups in the `\"bankrupt\"` column. Remember to label your axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044597ce-013f-4985-83c2-827dfb926a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot\n",
    "sns.boxplot(x=\"bankrupt\", y=\"feat_27\",data=df)\n",
    "plt.xlabel(\"Bankrupt\")\n",
    "plt.ylabel(\"POA / financial expenses\")\n",
    "plt.title(\"Distribution of Profit/Expenses Ratio, by Class\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023bdc3c-182e-43da-a77c-4ed8f20a7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for `feat_27`\n",
    "df[\"feat_27\"].describe().apply(\"{0:,.0f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33e8e4-331d-4155-af73-de5a6f26636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of `feat_27`\n",
    "df[\"feat_27\"].hist()\n",
    "plt.xlabel(\"POA / financial expenses\")\n",
    "plt.ylabel(\"Count\"),\n",
    "plt.title(\"Distribution of Profit/Expenses Ratio\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60e0bd-480f-406c-a8d3-d747241ac880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clipped boxplot\n",
    "q1,q9 = df[\"feat_27\"].quantile([0.1,0.9])\n",
    "mask = df[\"feat_27\"].between(q1,q9)\n",
    "#mask.head()\n",
    "sns.boxplot(x=\"bankrupt\", y=\"feat_27\", data=df[mask])\n",
    "plt.xlabel(\"Bankrupt\")\n",
    "plt.ylabel(\"POA / financial expenses\")\n",
    "plt.title(\"Distribution of Profit/Expenses Ratio, by Bankruptcy Status\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c57bf7-2893-428b-8461-c23ff95d23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.drop(columns=\"bankrupt\").corr()\n",
    "#corr.head()\n",
    "sns.heatmap(corr);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8909ff-2b36-4c03-b687-d3b48b47fdf1",
   "metadata": {},
   "source": [
    "So what did we learn from this EDA? First, our data is imbalanced. This is something we need to address in our data preparation. Second, many of our features have missing values that we'll need to impute. And since the features are highly skewed, the best imputation strategy is likely median, not mean. Finally, we have autocorrelation issues, which means that we should steer clear of linear models, and try a tree-based model instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d3897-9c93-41f4-83f4-f0d0afb342c7",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b19eb3c-1230-424b-8764-4f6a06edc666",
   "metadata": {},
   "source": [
    "So let's start building that model. If you need a refresher on how and why we split data in these situations, take a look back at the Time Series module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee3cc2b-7277-4072-b059-51604ab16119",
   "metadata": {},
   "source": [
    " Create your feature matrix `X` and target vector `y`. Your target is `\"bankrupt\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e4a58-cab3-44ca-aa76-60658798844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"bankrupt\"\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1c33ff-3c09-4f7b-ae0c-d6d4a097d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,y, test_size = 0.2, random_state = 42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc3cedc-b629-4d44-aeac-a8e6abc1d4bf",
   "metadata": {},
   "source": [
    "## Resample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c3100-3479-4faa-b4cb-c4ccaf7f2ef5",
   "metadata": {},
   "source": [
    "Now that we've split our data into training and validation sets, we can address the class imbalance we saw during our EDA. One strategy is to resample the training data. (This will be different than the resampling we did with time series data ) There are many to do this, so let's start with under-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b329ad-7930-4a22-84eb-b4eba49ea5db",
   "metadata": {},
   "source": [
    "Create a new feature matrix `X_train_under` and target vector `y_train_under` by performing random under-sampling on your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ac1d0-ca6e-4daf-8d53-eb356ada12b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_train_under, y_train_under = under_sampler.fit_resample(X_train,y_train)\n",
    "print(X_train_under.shape)\n",
    "X_train_under.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bdc3f9-5493-458b-8cc7-428917d0c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_under.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c92307-b2eb-468c-9649-ef49cc514282",
   "metadata": {},
   "source": [
    " Create a new feature matrix `X_train_over` and target vector `y_train_over` by performing random over-sampling on your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69a02e-beae-4771-906c-d8f86f157038",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_sampler = RandomOverSampler(random_state=42)\n",
    "X_train_over, y_train_over = over_sampler.fit_resample(X_train,y_train)\n",
    "print(X_train_over.shape)\n",
    "X_train_over.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d594ad-728f-441c-801e-69e6db45de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_over.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8888e4-c5ae-40bc-8526-f8e9da59610a",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2cd15-cb93-4e66-9bf3-7e8e2e31d4dc",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9698d28b-001c-4305-ab77-ef2b57fe1a16",
   "metadata": {},
   "source": [
    "As always, we need to establish the baseline for our model. Since this is a classification problem, we'll use accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7e25f-c82f-46f0-beaa-10d484b19b82",
   "metadata": {},
   "source": [
    "Calculate the baseline accuracy score for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fcd7b-45b6-4d7f-bfad-ba20f14d165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_baseline = y_train.value_counts(normalize=True).max()\n",
    "print(\"Baseline Accuracy:\", round(acc_baseline, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6e71c6-5ba6-40e0-9475-61f4b84a1153",
   "metadata": {},
   "source": [
    "Note here that, because our classes are imbalanced, the baseline accuracy is very high. We should keep this in mind because, even if our trained model gets a high validation accuracy score, that doesn't mean it's actually good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7df204-a20c-4e16-8dfe-46de793b024e",
   "metadata": {},
   "source": [
    "## Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b2c81-d762-4579-b579-4cf2be3f9acf",
   "metadata": {},
   "source": [
    "Now that we have a baseline, let's build a model to see if we can beat it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce700272-ac67-4088-8ff7-5bbe819ac692",
   "metadata": {},
   "source": [
    "Create three identical models: `model_reg`, `model_under` and `model_over`. All of them should use a `SimpleImputer` followed by a `DecisionTreeClassifier`. Train `model_reg` using the unaltered training data. For `model_under`, use the undersampled data. For `model_over`, use the oversampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77a82e-b856-4a3e-ac8a-a5443febc869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on `X_train`, `y_train`\n",
    "model_reg = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    DecisionTreeClassifier(random_state=42)\n",
    ")\n",
    "model_reg.fit(X_train, y_train)\n",
    "\n",
    "# Fit on `X_train_under`, `y_train_under`\n",
    "model_under = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    DecisionTreeClassifier(random_state=42)\n",
    ")\n",
    "model_under.fit(X_train_under, y_train_under)\n",
    "\n",
    "# Fit on `X_train_over`, `y_train_over`\n",
    "model_over = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    DecisionTreeClassifier(random_state=42)\n",
    ")\n",
    "model_over.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fcb0ab-5824-45ea-962b-3dead02ba3a4",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff7463-26be-42fe-903e-396c26a20826",
   "metadata": {},
   "source": [
    "How did we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f51ee8-d5fa-4e68-9bbd-fd78aaf10e8a",
   "metadata": {},
   "source": [
    " Calculate training and test accuracy for your three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5600e3da-9659-47a5-86ae-5212b5822233",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [model_reg, model_under, model_over]:\n",
    "    acc_train = m.score(X_train,y_train)\n",
    "    acc_test = m.score(X_test,y_test)\n",
    "\n",
    "    print(\"Training Accuracy:\", round(acc_train, 4))\n",
    "    print(\"Test Accuracy:\", round(acc_test, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b49bc6-6849-4c72-a419-d87e740e7641",
   "metadata": {},
   "source": [
    "As we mentioned earlier, \"good\" accuracy scores don't tell us much about the model's performance when dealing with imbalanced data. So instead of looking at what the model got right or wrong, let's see how its predictions differ for the two classes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65cc6d-eae5-4d8c-bdcd-08d8f37430e4",
   "metadata": {},
   "source": [
    "Plot a confusion matrix that shows how your best model performs on your validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85369c-7be1-48bb-af91-9442f2f11bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(model_reg, X_test,y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f9f47-aadf-43b1-9962-d4b9953c8e9d",
   "metadata": {},
   "source": [
    "Determine the depth of the decision tree in `model_over`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07115e70-a982-46f5-81e0-cf74c3352281",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = model_over.named_steps[\"decisiontreeclassifier\"].get_depth()\n",
    "print(depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314582a-53f9-4c9e-bd39-8b22c281e427",
   "metadata": {},
   "source": [
    "# Communicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a6c43-0411-40ca-a953-740aa8524454",
   "metadata": {},
   "source": [
    "Now that we have a reasonable model, let's graph the importance of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e44258-f21b-47fb-bd9b-6f2415f029bb",
   "metadata": {},
   "source": [
    "Create a horizontal bar chart with the 15 most important features for `model_over`. Be sure to label your x-axis `\"Gini Importance\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7985e57b-c6f0-4ff9-a55e-a8f05c340dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get importances\n",
    "importances = model_over.named_steps[\"decisiontreeclassifier\"].feature_importances_\n",
    "\n",
    "# Put importances into a Series\n",
    "feat_imp = pd.Series(importances, index=X_train_over.columns).sort_values()\n",
    "#feat_imp.head()\n",
    "# Plot series\n",
    "feat_imp.tail(15).plot(kind=\"barh\")\n",
    "plt.xlabel(\"Gini Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"model_over Feature Importance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b2c4b-e4c3-41b3-b02d-f3e717d805b6",
   "metadata": {},
   "source": [
    "There's our old friend `\"feat_27\"` near the top, along with features 34 and 26. It's time to share our findings.\n",
    "\n",
    "Sometimes communication means sharing a visualization. Other times, it means sharing the actual model you've made so that colleagues can use it on new data or deploy your model into production. First step towards production: saving your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b36ba6-2e9f-4255-bab9-a09e9d0d46cc",
   "metadata": {},
   "source": [
    "Using a context manager, save your best-performing model to a a file named `\"model-5-2.pkl\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48313310-9857-427e-b92d-5c2b217e1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model as `\"model-5-2.pkl\"`\n",
    "with open(\"model-5-2.pkl\",\"wb\") as f:\n",
    "    pickle.dump(model_over, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ee36e-9ba8-491f-8101-8512334f2ed6",
   "metadata": {},
   "source": [
    "Make sure you've saved your model correctly by loading `\"model-5-2.pkl\"` and assigning to the variable `loaded_model`. Once you're satisfied with the result, run the last cell to submit your model to the grader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263786f8-dc69-476f-b71d-6fa1e88cceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `\"model-5-2.pkl\"`\n",
    "with open(\"model-5-2.pkl\", \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39319d59-196c-4b69-8e83-4dc0e909dd70",
   "metadata": {},
   "source": [
    "- [If you feel that my work is insighful and allow me to work in your team, please visit my website.](https://makt96.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c046d57-d6c2-4772-a0d3-f521487702e9",
   "metadata": {},
   "source": [
    "## Thanks to see and evaluate my work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
